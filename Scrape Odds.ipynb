{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the odds\n",
    "Website: https://www.unibet.eu/betting#filter/football/\n",
    "\n",
    "From Unibet using Selenium and Firefox.\n",
    "\n",
    "Data is stored in `./data/This_months_odds.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the browser\n",
    "browser = webdriver.Firefox()\n",
    "\n",
    "# have a generic wait object for the driver\n",
    "wait = WebDriverWait(browser, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for page to be done loading\n",
    "def pageReady(interval=1):\n",
    "    # in seconds\n",
    "    waited = 0\n",
    "    while True:\n",
    "        soupCheck1 = BeautifulSoup(browser.page_source, 'html5lib')\n",
    "        waited += interval\n",
    "        sleep(interval)\n",
    "        soupCheck2 = BeautifulSoup(browser.page_source, 'html5lib')\n",
    "        if soupCheck1 == soupCheck2:\n",
    "            break\n",
    "    return waited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# go to Unibet\n",
    "url_0 = 'https://www.unibet.eu/betting#filter/football/'\n",
    "browser.get(url_0)\n",
    "\n",
    "# wait\n",
    "print('Page ready in', pageReady())\n",
    "\n",
    "# click on the cookie dialog\n",
    "browser.find_element(By.ID, \"CybotCookiebotDialogBodyButtonAccept\").click()\n",
    "\n",
    "# wait again\n",
    "print('Page ready in', pageReady())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the leagues and store the page source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the urls we'll visit\n",
    "urls = [url_0 + u for u in ['netherlands/eredivisie',\n",
    "                            'spain/la_liga',\n",
    "                            'germany/bundesliga',\n",
    "                            'england/premier_league',\n",
    "                            'france/ligue_1',\n",
    "                            'italy/serie_a']]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty dict to append the soup of each league\n",
    "soups = dict()\n",
    "\n",
    "# loop over all urls\n",
    "for url in urls:\n",
    "    \n",
    "    # debug\n",
    "    #url = urls[0]\n",
    "    \n",
    "    # go to url\n",
    "    print('Going to:', url)\n",
    "    browser.get(url)\n",
    "    \n",
    "    # wait until page is ready\n",
    "    print('Page ready in', pageReady())\n",
    "\n",
    "    # click on all the dropdown elements, ignore the already expanded ones\n",
    "    dropdowns = browser.find_elements(By.CSS_SELECTOR,\n",
    "        '.KambiBC-collapsible-container:not(.KambiBC-expanded)')\n",
    "    for dropdown in dropdowns:\n",
    "        dropdown.click()\n",
    "\n",
    "    # wait again\n",
    "    print('Page ready in', pageReady())\n",
    "\n",
    "    # now get the beautiful source code\n",
    "    soup = BeautifulSoup(browser.page_source, 'html5lib')\n",
    "    \n",
    "    # select and store only the list of matches\n",
    "    league = re.split('/', url)[-1]\n",
    "    soups[league] = soup.select_one('.KambiBC-event-groups-list')\n",
    "    \n",
    "    # debug\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the source code. From here on we won't need the browser anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the soups we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcn: extract match info from a <li.KambiBC-event-item>\n",
    "def extractMatchInfo(match):\n",
    "    teams = match.select('.KambiBC-event-participants__name')\n",
    "    odds = match.select('span.KambiBC-mod-outcome__odds')\n",
    "    info = {'home_team': teams[0].text,\n",
    "            'away_team': teams[1].text,\n",
    "            'odd_home_win': odds[0].text,\n",
    "            'odd_tie': odds[1].text,\n",
    "            'odd_away_win': odds[2].text\n",
    "           }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a main data frame to store all info in\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop over all soups\n",
    "for league, soup in soups.items():\n",
    "    \n",
    "    # find all green buttons (with odds)\n",
    "    greenButtons = soup.select('button.KambiBC-mod-outcome')\n",
    "    \n",
    "    # find the <li> parent which represents one match and combine them as a set\n",
    "    matches = list(set(o.find_parent('li', class_='KambiBC-event-item') for o in greenButtons))\n",
    "\n",
    "    # extract info of all matches and store in a data frame\n",
    "    df_0 = pd.DataFrame([extractMatchInfo(m) for m in matches])\n",
    "    \n",
    "    # add the league name to the data frame\n",
    "    df_0['league'] = league\n",
    "    print(league, ':', df_0.shape)\n",
    "    \n",
    "    # append\n",
    "    df = pd.concat([df, df_0])\n",
    "    \n",
    "    # debug\n",
    "    #break\n",
    "\n",
    "# show\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some text transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change team names to lowercase\n",
    "df[\"home_team\"] = df[\"home_team\"].str.lower()\n",
    "df[\"away_team\"] = df[\"away_team\"].str.lower()\n",
    "\n",
    "# remove all accents\n",
    "# https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "df[\"home_team\"] = df[\"home_team\"].str.normalize('NFKD').str.encode(encoding='ascii',errors='ignore').str.decode('utf-8')\n",
    "df[\"away_team\"] = df[\"away_team\"].str.normalize('NFKD').str.encode(encoding='ascii',errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# convert to numbers\n",
    "df[\"odd_home_win\"] = pd.to_numeric(df[\"odd_home_win\"])\n",
    "df[\"odd_away_win\"] = pd.to_numeric(df[\"odd_away_win\"])\n",
    "df[\"odd_tie\"] = pd.to_numeric(df[\"odd_tie\"])\n",
    "\n",
    "# show\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df.to_csv('./data/This_months_odds.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
